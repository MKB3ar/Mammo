# model_clean.py â€“ Â«Ñ‡Ğ¸ÑÑ‚Ñ‹Ğ¹Â» Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ Ğ±ĞµĞ· stemâ€‘maxpool
# ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ¿Ñ€Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğµ 512Ã—512 Ğ¸ ĞºĞ¾Ğ¼Ñ„Ğ¾Ñ€Ñ‚Ğ½Ğ¾ Ğ¿Ğ¾Ğ¼ĞµÑ‰Ğ°ĞµÑ‚ÑÑ Ğ² 8â€¯Ğ“Ğ‘ VRAM

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision.models import resnet34, ResNet34_Weights


# â”€â”€ Encoder â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _make_1ch_resnet34(pretrained: bool = True):
    """ResNetâ€‘34 Ñ Ğ¾Ğ´Ğ½Ğ¾ĞºĞ°Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ñ…Ğ¾Ğ´Ğ¾Ğ¼ Ğ¸ **strideâ€¯=â€¯2** (Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµĞ¼).
    ĞœÑ‹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑƒĞ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿ÑƒĞ» Ğ¿Ğ¾ÑĞ»Ğµ conv1, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ñ„Ğ¸Ñ‡Ğ° Ğ±Ñ‹Ğ»Ğ°
    256â€¯Ã—â€¯256 (Ğ° Ğ½Ğµ 128â€¯Ã—â€¯128). Ğ­Ñ‚Ğ¾ ÑƒĞ´Ğ²Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ skipâ€‘ÑĞ²ÑĞ·Ğ¸ f0,
    Ğ½Ğ¾ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ½Ğµ Ğ±ÑŒÑ‘Ñ‚ Ğ¿Ğ¾ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ (â‰ˆâ€¯+0.4â€¯Ğ“Ğ‘ Ğ¿Ñ€Ğ¸ batch=8, fp16).
    """
    m = resnet34(weights=ResNet34_Weights.IMAGENET1K_V1 if pretrained else None)
    w = m.conv1.weight.data.mean(1, keepdim=True)
    m.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)
    m.conv1.weight.data = w
    return m


class Encoder(nn.Module):
    def __init__(self, pretrained: bool = True):
        super().__init__()
        m = _make_1ch_resnet34(pretrained)
        # ğŸ”» ÑƒĞ±Ñ€Ğ°Ğ»Ğ¸ m.maxpool â†’ f0 = 64ch Ã— 256Â²
        self.stem = nn.Sequential(m.conv1, m.bn1, m.relu)
        self.l1, self.l2, self.l3, self.l4 = m.layer1, m.layer2, m.layer3, m.layer4

    def forward(self, x):
        f0 = self.stem(x)     # 64 @ 256Â² (Ğ±Ñ‹Ğ»Ğ¾ 128Â²)
        f1 = self.l1(f0)      # 64 @ 256Â²
        f2 = self.l2(f1)      # 128 @ 128Â²
        f3 = self.l3(f2)      # 256 @ 64Â²
        f4 = self.l4(f3)      # 512 @ 32Â²
        return f0, f1, f2, f3, f4


# â”€â”€ Decoder blocks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class DecoderBlock(nn.Module):
    """Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹ Ğ±Ğ»Ğ¾Ğº: â†‘2Ã— + concat(skip) + 2Ã—3Ã—3 conv"""

    def __init__(self, in_c: int, skip_c: int, out_c: int):
        super().__init__()
        self.up = nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2)
        self.conv = nn.Sequential(
            nn.Conv2d(out_c + skip_c, out_c, 3, padding=1),
            nn.BatchNorm2d(out_c), nn.ReLU(inplace=True),
            nn.Conv2d(out_c, out_c, 3, padding=1),
            nn.BatchNorm2d(out_c), nn.ReLU(inplace=True),
        )

    def forward(self, x, skip):
        x = self.up(x)                # â†‘2Ã— Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ñƒ
        x = torch.cat([x, skip], 1)   # concat
        return self.conv(x)


class DecoderBlockNoUp(nn.Module):
    """ĞŸĞ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ: **Ğ±ĞµĞ·** Ğ°Ğ¿ÑÑĞ¼Ğ¿Ğ»Ğ°, Ñ‚.Ğº. f0 ÑƒĞ¶Ğµ Ñ‚Ğ°ĞºĞ¾Ğ³Ğ¾ Ğ¶Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°."""

    def __init__(self, in_c: int, skip_c: int, out_c: int):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_c + skip_c, out_c, 3, padding=1),
            nn.BatchNorm2d(out_c), nn.ReLU(inplace=True),
            nn.Conv2d(out_c, out_c, 3, padding=1),
            nn.BatchNorm2d(out_c), nn.ReLU(inplace=True),
        )

    def forward(self, x, skip):
        x = torch.cat([x, skip], 1)
        return self.conv(x)


# â”€â”€ Full network â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class MammoNet(nn.Module):
    def __init__(self, n_classes: int = 4, n_birads: int = 6, pretrained: bool = True):
        super().__init__()
        self.enc = Encoder(pretrained)

        self.dec4 = DecoderBlock(512, 256, 256)   # 32Â² â†’ 64Â²
        self.dec3 = DecoderBlock(256, 128, 128)   # 64Â² â†’ 128Â²
        self.dec2 = DecoderBlock(128,  64,  64)   # 128Â² â†’ 256Â²
        # ğŸ”» Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğ¹ Ğ±Ğ»Ğ¾Ğº Ğ±ĞµĞ· Ğ°Ğ¿ÑÑĞ¼Ğ¿Ğ»Ğ° (256Â² Ğ¾ÑÑ‚Ğ°Ñ‘Ñ‚ÑÑ 256Â²)
        self.dec1 = DecoderBlockNoUp(64, 64, 32)

        self.seg_head = nn.Conv2d(32, 1, kernel_size=1)

        self.gpool   = nn.AdaptiveAvgPool2d(1)
        self.drop    = nn.Dropout(0.30)          # â† Ğ½Ğ¾Ğ²Ğ¾Ğµ v2
        
        self.cls_fc  = nn.Linear(512, n_classes)
        self.br_fc   = nn.Linear(512, n_birads - 1)

    # ------------------------------------------------------------------
    def forward(self, x):
        f0, f1, f2, f3, f4 = self.enc(x)

        d = self.dec4(f4, f3)
        d = self.dec3(d, f2)
        d = self.dec2(d, f1)
        d = self.dec1(d, f0)           # Ğ¸Ñ‚Ğ¾Ğ³ 256Â²

        seg = torch.sigmoid(self.seg_head(d))
        # â†‘ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ¿ÑÑĞ¼Ğ¿Ğ» Ğ´Ğ¾ 512Â² â€“ Ğ±Ğ¸Ğ»Ğ°Ñ‚ĞµÑ€Ğ°Ğ»ÑŒĞ½Ğ¾, Ğ±ĞµĞ· Ñ€Ğ°Ğ·Ğ¼Ñ‹Ñ‚Ğ¸Ñ ÑƒĞ³Ğ»Ğ¾Ğ²
        seg = F.interpolate(seg, scale_factor=2, mode="bilinear", align_corners=False)

        g = self.gpool(f4).flatten(1)
        logits_cls = self.cls_fc(self.drop(g))   # â† Ğ¾Ğ±Ğ¾Ñ€Ğ°Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ½Ğ¾Ğ²Ğ¾Ğµ v2

        logits_cls = self.cls_fc(g)
        logits_br  = self.br_fc(g)
        return seg, logits_cls, logits_br
